{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Inter-Rater Reliability\n",
    "\n",
    "This notebook contains the calculation of three versions of inter-rater reliability. The scores give insight into how reliable the coding process of the first author are by comparing it to the independent coding process of the second rater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of the extractions coded by the second rater, i.e., indices of the extractions for which an overlap exist\n",
    "overlap = [10, 27, 50, 58, 63, 69, 86, 92, 96]\n",
    "\n",
    "with open('./../../data//columns.json', 'r') as f:\n",
    "    columns = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juf\\Workspace\\BTH\\Data Processing\\rqi-pat\\.venv\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\juf\\Workspace\\BTH\\Data Processing\\rqi-pat\\.venv\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "data_location = './../../data/interview-data.xlsx'\n",
    "\n",
    "df_orig = pd.read_excel(data_location, \n",
    "                        sheet_name='Data',\n",
    "                        usecols=[column['name'] for column in columns],\n",
    "                        dtype={'Impact 1': \"string\", 'Impact 2': \"string\"}) \\\n",
    "    .set_index('ID') \\\n",
    "    .filter(items=overlap, axis = 0) \\\n",
    "    .fillna('na')\n",
    "\n",
    "df_overlap = pd.read_excel(data_location, \n",
    "                           sheet_name='Overlap',\n",
    "                        usecols=[column['name'] for column in columns],\n",
    "                        dtype={'Impact 1': \"string\", 'Impact 2': \"string\"}) \\\n",
    "    .set_index('ID') \\\n",
    "    .filter(items=overlap, axis = 0) \\\n",
    "    .fillna('na')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage Agreement\n",
    "\n",
    "Percentage agreement is the simplest type of inter-rater reliability. It suffers from the fact that it does not account for agreement caused by chance.\n",
    "\n",
    "Holsti, O. R. (1969). Content analysis for the social sciences and humanities. Reading. MA: Addison-Wesley (content analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent agreement of 83.76% (98/117)\n"
     ]
    }
   ],
   "source": [
    "individual_labels = 0\n",
    "matching_labels = 0\n",
    "\n",
    "for paperid in overlap:\n",
    "    for column in df_orig.columns:\n",
    "        individual_labels += 1\n",
    "        if df_orig.loc[paperid][column] == df_overlap.loc[paperid][column]: \n",
    "            matching_labels += 1\n",
    "\n",
    "print(f'Percent agreement of {matching_labels/individual_labels:.2%} ({matching_labels}/{individual_labels})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa\n",
    "\n",
    "Cohen's Kappa accounts for agreement caused by chance but samples the expected marginal distributions from the data directly, which is not applicable in this labeling task. The measure is calculated for completeness' and comparison's sake.\n",
    "\n",
    "Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1), 37-46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cohen's Kappa of 71.78%\n"
     ]
    }
   ],
   "source": [
    "codes = {column['name']: column['codes']+['na'] for column in columns[1:]}\n",
    "kappas = {}\n",
    "\n",
    "for column in df_orig.columns:\n",
    "    y1 = df_orig[column].to_list()\n",
    "    y2 = df_overlap[column].to_list()\n",
    "    labels = codes[column]\n",
    "\n",
    "    kappa = cohen_kappa_score(y1, y2, labels=labels)\n",
    "    if math.isnan(kappa):\n",
    "        kappa = 1\n",
    "\n",
    "    kappas[column] = kappa\n",
    "\n",
    "\n",
    "kappa_values = [kappas[column] for column in kappas]\n",
    "avg_kappa = sum(kappa_values)/len(kappa_values)\n",
    "\n",
    "print(f\"Average Cohen's Kappa of {avg_kappa:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bennett's S-Score\n",
    "\n",
    "Bennett's S-score is a recommended alternative to Cohen's Kappa in this task, since it does account for agreement caused by chance but does not make the same assumptions as described above.\n",
    "\n",
    "Bennett, E. M., Alpert, R., & Goldstein, A. C. (1954). Communications through limited-response questioning. Public Opinion Quarterly, 18(3), 303-308."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bennet's S-Score of 82.30%\n"
     ]
    }
   ],
   "source": [
    "def equal_proportion(y1, y2):\n",
    "    D = 0\n",
    "    for e1, e2 in zip(y1, y2):\n",
    "        if e1 == e2:\n",
    "            D += 1\n",
    "\n",
    "    return D/len(y1)\n",
    "\n",
    "sscores = {}\n",
    "\n",
    "def bennetts_s_score(y1, y2, labels):\n",
    "    k = len(labels)\n",
    "    p = equal_proportion(y1, y2)\n",
    "    s = (k/(k-1)) * (p-(1/k))  \n",
    "    return s\n",
    "\n",
    "for column in df_orig.columns:\n",
    "    y1 = df_orig[column].to_list()\n",
    "    y2 = df_overlap[column].to_list()\n",
    "    labels = codes[column]\n",
    "\n",
    "    sscores[column] = bennetts_s_score(y1, y2, labels)\n",
    "\n",
    "s_score_values = [sscores[column] for column in sscores]\n",
    "avg_sscore = sum(s_score_values)/len(s_score_values)\n",
    "print(f\"Average Bennet's S-Score of {avg_sscore:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
